id,Title,Contact Name,Contact Email,abstract,authors,url
P1204,ARpenSki: Augmenting Ski Training with Direct and Indirect Postural Visualization,Takashi Matsumoto,matsumoto.t.bb@m.titech.ac.jp,"Alpine skiing is a popular winter sport, and several systems have been proposed to enhance training and improve efficiency. However, many existing systems rely on simulation-based environments, which suffer from drawbacks such as a gap between real skiing and the lack of body ownership.  To address these limitations, we present ARpenSki, a novel augmented reality (AR) ski training system that employs a see-through head mounted display (HMD) to deliver augmented visual training cues that may be applied on real slopes. The proposed AR system provides a transparent view of the lower half of the field of vision, where we implemented three different AR-based direct and indirect postural visualization methods.",Takashi Matsumoto: Tokyo Institute of Technology; Erwin Wu: Tokyo Institute of Technology; Chen-Chieh Liao: Tokyo Institute of Technology; Hideki Koike: Tokyo Institute of Technology,
P1978,Evaluating an In-Hand Ball-Shaped Controller for Object Manipulation in Virtual Reality,Sunbum Kim,ksb4587@kaist.ac.kr,"This study explored the use of a ball-shaped controller for object manipulation in virtual reality. We developed a ball-shaped controller with pressure-sensing capabilities featuring specifically designed interactions for object manipulation, including selection, translation, rotation, and scaling. We evaluated it on tasks involving both 6-DOF and 7-DOF object manipulation, including close and distant ranges. The results indicated that the ball-shaped controller performed similarly to VR controller methods for direct manipulation but excelled in reducing completion times and task load for distant object manipulation. Additionally, the ball-shaped controller minimized wrist and arm movements and was the preferred method among participants.","Sunbum Kim: School of Computing, KAIST; Geehyuk Lee: School of Computing, KAIST",
P1817,MeshReduce: Scalable and Bandwidth Efficient 3D Scene Capture,Tao Jin,taojin@andrew.cmu.edu,"This paper introduces MeshReduce, a system for bandwidth-efficient and scalable 3D scene capture and streaming. It addresses the high latency, bandwidth, and scalability issues of traditional methods by employing sensor-side compute nodes for per-sensor mesh reconstruction, decimation, and compression. MeshReduce also introduces an innovative network rate control. MeshReduce efficiently merges independent mesh reconstructions, ensuring adaptive, high-quality streaming. Demonstrated with Azure Kinect depth camera and a custom LiDAR + 360° camera setup, MeshReduce offers a scalable and efficient solution for 3D scene capture and rendering.",Tao Jin: Carnegie Mellon University; Mallesham Dasari: Carnegie Mellon University; Connor Smith: NVIDIA; Kittipat Apicharttrisorn: Nokia Bell Labs; Srinivasan Seshan: Carnegie Mellon University; Anthony Rowe: Carnegie Mellon University,
P1061,A SharpView Font with Enhanced Out-of-Focus Text Legibility for Augmented Reality Systems,Mohammed Safayet Arefin,arefin@acm.org,"In optical see-through augmented reality system, users are required to frequently change their eye focus from one distance to another to view virtual and real information at different distances, and only one piece of information is in focus while the other is out of focus. This paper introduces a novel SharpView font, which looks sharper and more legible than standard fonts when seen out of focus. Our method models out-of-focus blur with Zernike mathematical formulations, develops a focus correction algorithm, and proposes a novel sharpness quantification algorithm. Simulation and camera-based measurement showed that SharpView font is significantly sharper than standard fonts.",Mohammed Safayet Arefin: Colorado State University; Carlos Montalto: University of Costa Rica; Alexander Plopski: TU Graz; J. Edward Swan II: Mississippi State University,
P1265,Hap'n'Roll: A Scroll-inspired Device for Delivering Diverse Haptic Feedback with a Single Actuator,Hiroki Ota,tesula22@gmail.com,"Hap'n'Roll is an innovative tactile device with a scroll-like mechanism and a single motor that manipulates a sheet to simulate various shapes and textures. This unique approach enables the presentation of multiple forms beyond cylindrical shapes and the conveyance of different textures to the user's fingertips. User studies confirm Hap'n'Roll's effectiveness, with high shape recognition accuracy, showcasing its potential in diverse haptic feedback applications.",Hiroki Ota: Nara Institute of Science and Technology; Daiki Hagimori: Nara Institute of Science and Technology; Monica Perusquia-Hernandez: Nara Institute of Science and Technolgy; Naoya Isoyama: Otsuma Women's University; Yutaro Hirao: Nara Institute of Science and Technology; Hideaki Uchiyama: Nara Institute of Science and Technology; Kiyoshi Kiyokawa: Nara Institute of Science and Technology,
P1432,Jump Cut Effects in Cinematic Virtual Reality: Editing with the 30-degree Rule and 180-degree Rule,Junjie Zhang,jakezhang@hkust-gz.edu.cn,"This study pioneers the examination of jump cuts' influence on presence, viewing experience, and edit quality in cinematic virtual reality (VR). Employing the 30-degree and 180-degree rules, our user study with thirteen participants reveals substantial improvements in presence, viewing experience, and edit quality. These findings offer crucial insights for VR content creators and editors seeking to elevate the effectiveness and immersiveness of VR experiences. In response to the escalating demand for top-tier VR content, this research provides valuable guidance for those navigating the evolving landscape of VR editing techniques.",Junjie Zhang: The Hong Kong University of Science and Technology; Lik-Hang Lee: Hong Kong Polytechnic University; Yuyang Wang: Hong Kong University of Science and Technology; Shan Jin: The Hong Kong University of Science and Technology (Guangzhou); Dan-Lu Fei: Hong Kong University of Science and Technology; Pan Hui: The Hong Kong University of Science and Technology,
P1909,Analyzing Cognitive Demands and Detection Thresholds for Redirected Walking in Immersive Forest and Urban Environments,Fariba Mostajeran,fariba.mostajeran.gourtani@uni-hamburg.de,"The redirected walking technique allows users to explore larger spaces in immersive virtual environments (IVEs) via natural walking compared to their physical spaces. However, it may induce extra cognitive load for its users. On the other hand, it has been shown that the type of IVE can restore or diminish users' attentional capacities. In this paper, we examine the effects of two types of IVE (i.e., forest and urban) on redirected walking and its cognitive demands. The results of our frequentist and Bayesian analysis were consistent and suggest that redirected walking is robust to the variation of the tested IVEs.",Fariba Mostajeran: Universität Hamburg; Sebastian Schneider: Universität Hamburg; Gerd Bruder: University of Central Florida; Simone Kühn: Max Planck Institute for Human Development; Frank Steinicke: Universität Hamburg,
P1348,Excuse Me: Large Groups in Small Rooms,Ephraim Schott,ephraim.schott@uni-weimar.de,"Standing in a large crowd can be uncomfortable and usually results in other users obstructing the view of the virtual environment. In this paper, we present four techniques designed to improve the user’s view in crowded environments. Inspired by related work on various transparency techniques, as well as observed user behavior in crowded scenarios, this paper addresses the visibility problem by locally manipulating the appearance of other users. A user study with 24 participants found that the transparency technique was advantageous for quick search tasks. However, in a realistic museum setting, no clear favorite could be determined as the techniques make different trade-offs and users weighted these aspects differently.",Ephraim Schott: Bauhaus-Universität Weimar; Tony Jan Zoeppig: Bauhaus-Universität Weimar; Anton Benjamin Lammert: Bauhaus-Universität Weimar; Bernd Froehlich: Bauhaus-Universität Weimar,
P1922,Age and Realism of Avatars in Simulated Augmented Reality: Experimental Evaluation of Anticipated User Experience,Veronika Mikhailova,veronika.mikhailova@tu-ilmenau.de,"The study investigates the social attractiveness of avatars in simulated augmented reality (AR) based on a sample of N=2086 age-diverse participants from Germany. In an online setting, participants evaluated avatars representing different age groups (younger, middle-aged, older) and levels of realism (low, medium, high). Results demonstrated a strong preference for younger, high-realism avatars as communication partners and for self-representation in AR. However, older adults showed a tendency to opt for avatars resembling their actual age. The study provides insights into social interactions in AR, highlighting age-related stereotypes in avatar-based communication and underscoring the need for a more inclusive avatar design.",Veronika Mikhailova: Technische Universität Ilmenau; Christoph Gerhardt: Technische Universität Ilmenau; Christian Kunert: Technische Universität Ilmenau; Tobias Schwandt: Technische Universität Ilmenau; Florian Weidner: Technische Universität Ilmenau; Wolfgang Broll: Technische Universität Ilmenau; Nicola Döring: Technische Universität Ilmenau,
P1633,HardenVR: Harassment Detection in Social Virtual Reality,Na Wang,nwang4@gmu.edu,"Despite the promising prospects of social VR, there is a growing concern about the harassment issue. Existing protections against harassment are highly limited. The deficiency of studies further complicates the situation. To address these challenges, we first build a customized platform to collect data about users’ social interaction behaviors. An analysis of the collected dataset reveals harassment detection depends not only on users’ actions but also on the spatial and temporal relationships. In order to accurately discern harassment, we propose the novel context-aware framework HardenVR, which employs a transformer-based model to learn hand actions and relative poses. The experiments show its accuracy as high as 98.26%.",Na Wang: George Mason University; Jin Zhou: George Mason University; Jie Li: EPAM ; Bo Han: George Mason University; Fei Li: George Mason University; Songqing Chen: George Mason University,
P1942,OdorAgent: Generate Odor Sequences for Movies Based on Large Language Model,Yu Zhang,yu-zhang21@mails.tsinghua.edu.cn,"Numerous studies have shown that integrating scents into movies enhances viewer engagement and immersion. However, creating such olfactory experiences often requires professional perfumers to match scents, limiting their widespread use. To address this, we propose OdorAgent which combines a LLM with a text-image model to automate video-odor matching. The generation framework is in four dimensions: subject matter, emotion, space, and time. We applied it to a specific movie and conducted user studies to evaluate and compare the effectiveness of different system elements. The results indicate that OdorAgent possesses significant scene adaptability and enables inexperienced individuals to design odor experiences for video and images.",Yu Zhang: Tsinghua University; Peizhong Gao: Tsinghua University; Fangzhou Kang: Tsinghua Univerity; Jiaxiang Li: Tsinghua University; Jiacheng Liu: Tsinghua University; Qi Lu: Tsinghua University; YINGQING XU: Tsinghua University,
P1245,Effect of Hand and Object Visibility in Navigational Tasks Based on Rotational and Translational Movements in Virtual Reality,Amal Hatira,amal.hatira@stu.khas.edu.tr,"In this paper, we investigate the effect of hand avatar and object visibility on navigational tasks using a VR headset. Participants navigated a cylindrical object through virtual obstacles using rotational or translational movements. The study used three visibility conditions for the hand avatar (opaque, transparent, and invisible) and two conditions for the object (opaque and transparent). Results showed that participants performed faster and with fewer collisions using invisible and transparent hands and opaque objects and preferred the combination of transparent hands and opaque objects. The findings could help researchers and developers determine the visibility/transparency conditions for precise navigational tasks.",Amal Hatira: Kadir Has University; Zeynep Ecem Gelmez: Kadir Has University; Anil Ufuk Batmaz: Concordia University; Mine Sarac: Kadir Has University,
P1843,"Whatever could be, could be: Visualizing Future Movement Predictions",Chenkai Zhang,chenkai.zhang@mymail.unisa.edu.au,"As technology grants us superhuman powers, looking into what the future may hold is no longer science fiction. We present our work evaluating visualizations of future predictions in the Football domain. We explore the problem space, examining what a future may be. Three visualizations (2 arrow lines, 5 arrow lines, and a heatmap) are introduced as representations. Whilst football is used as an example domain in this work, the findings aim to generalize to other scenarios that contain trajectory information. Two VR studies (2xn=24) examined the visualizations in various situations. Results show heatmap as the most effective and preferred by the vast majority of participants. Findings offer insights into future visualization.",Chenkai Zhang: University of South Australia; Ruochen Cao: Taiyuan University of Technology; Andrew Cunningham: University of South Australia; James A. Walsh: University of South Australia,
P1828,Augmented Coach: Volumetric Motion Annotation and Visualization for Immersive Sports Coaching,Jiqing Wen,jwen31@asu.edu,"Traditional methods of remote sports coaching is challenged by the absence of spatial analysis capabilities, which hinders in-depth assessment of athletic performance. This paper introduces Augmented Coach, an immersive and interactive sports coaching system. Augmented Coach utilizes volumetric data to reconstruct the 3D representations of the athletes. As a result, coaches can not only view the resulting point cloud videos of the athletes performing athletic movements, but also employ the system's spatial annotation and visualization tools to gain insights into movement patterns and communicate with remote athletes.",Jiqing Wen: Arizona State University; Lauren Gold: Arizona State University; Qianyu Ma: Arizona State University; Robert LiKamWa: Arizona State University,
P2080,Real-Virtual Objects: Exploring Bidirectional Embodied Tangible Interaction with a Virtual Human in World-Fixed Virtual Reality,"Lal ""Lila"" Bozgeyikli",lboz@arizona.edu,"This paper explores bidirectional embodied tangible interaction between a human and a virtual human through shared objects that span the real-virtual boundary in world-fixed virtual reality. The shared objects extend from the real world into the virtual world (and vice versa). We discuss the novel interaction concept and implementation details and present the results of a between-subjects user study with 40 participants where we compared the developed novel real-virtual shared object interaction with a control version. The results showed that presence and co-presence were increased with the real-virtual object interaction, along with affective attraction to the virtual human and enjoyment of interaction.","Lal ""Lila"" Bozgeyikli: University of Arizona",
P1564,Context-Aware Head-and-Eye Motion Generation with Diffusion Model,Yuxin Shen,shenyuxin@bit.edu.cn,"We introduce a novel two-stage approach to generate context-aware head-and-eye motions across diverse scenes. By harnessing the capabilities of advanced diffusion models, our approach adeptly produces contextually appropriate eye gaze points, further leading to the generation of natural head-and-eye movements. Utilizing Head-Mounted Display (HMD) eye-tracking technology, we also present a comprehensive dataset, which captures human eye gaze behaviors in tandem with associated scene features. We show that our approach consistently delivers intuitive and lifelike head-and-eye motions and demonstrates superior performance in terms of motion fluidity, alignment with contextual cues, and overall user satisfaction.",Yuxin Shen: Yangtze Delta Region Academy of Beijing Institute of Technology; Manjie Xu: Beijing institute of technology; Wei Liang: Beijing Institute of Technology,
P1509,Virtual Steps: The Experience of Walking for a Lifelong Wheelchair User in Virtual Reality,Atieh Taheri,atieh@ece.ucsb.edu,"We co-designed a VR walking experience with a person with Spinal Muscular Atrophy who has been a lifelong wheelchair user. Over 9 days, we collected and analyzed data on this person's experience through a diary study to understand the required design elements. Given that they had only seen others walking and had not directly experienced it, determining which design parameters must be considered to match the virtual experience to their mental model was challenging. Generally, we found the experience of walking to be quite positive, providing a perspective from a higher vantage point than what was available in a wheelchair.","Atieh Taheri: University of California, Santa Barbara; Arthur Caetano: University of California, Santa Barbara; Misha Sra: UCSB",
P2100,Dynamic Scene Adjustment Mechanism for Manipulating User Experience in VR,Yi Li,elieli0925@qq.com,"Advancing VR tech creates realistic, controllable interactive environments, greatly impacting user experience. Concurrently, real-time user status monitoring advancements unlocked dynamic adjustments to VR environments through user real-time status and feedback. This paper introduces an interactive paradigm for VR environments called the Dynamic Scene Adjustment (DSA) mechanism, which can modify the VR environmental variables in real-time according to the user’s status and performance to enhance user engagement and experience. We adopted the perspective of visual environment variables, embedding the DSA mechanism into a music VR game with brain-computer interaction. Experimental results robustly support the rationality of the DSA approach.","Yi Li: Center for Future Media, the School of Computer Science and Engineering, University of Electronic Science and Technology of China; Zhitao Liu: UESTC; Li Yuan: Academy of Military Sciences; Haolan Tang: Center for Future Media, the School of Computer Science and Engineering, University of Electronic Science and Technology of China; YouTeng Fan: Center for Future Media, the School of Computer Science and Engineering, University of Electronic Science and Technology of China; Ning Xie: School of Computer Science and Engineering",
P1475,Enhancing Positive Emotions through Interactive Virtual Reality Experiences: An EEG-Based Investigation,Shiwei Cheng,swchengzjut@gmail.com,"Virtual reality (VR) holds potential to promote feelings of well-being by evoking positive emotions. Our study aimed to investigate the types of interaction behaviors in VR that effectively enhance positive emotions. An exploratory study (N = 22) was conducted on a virtual museum to study the impact of varying user autonomy and interaction behaviors on emotions. An individual emotion model based on electroencephalogram (EEG) was employed to predict the promotion of positive emotions. The results indicated that incorporating creative interaction functions increased positive emotions, with the extent of increase closely linked to the degree of user autonomy.",Shiwei Cheng: Zhejiang University of Technology; Sheng Danyi: Computer Science; Yuefan Gao: Cyborg Intelligence Technology Company; Zhanxun DONG: Shanghai Jiao Tong University; Ting Han: Shanghai Jiao Tong University,
P1309,"Eye-Hand Coordination Training: A Systematic Comparison of 2D, VR, and AR Screen Technologies and Task Motives",Aliza Aliza,aliza.rind@stu.khas.edu.tr,"In this paper, we compare user motor performance with an EHCT setup in Augmented Reality (AR), Virtual Reality (VR), and on a 2D touchscreen display in a longitudinal study. Through a ten-day user study, we thoroughly analyzed the motor performance of twenty participants with five task instructions focusing on speed, error rate, accuracy, precision, and none. As a novel evaluation criterion, we also analyzed the participants’ performance in terms of effective throughput. The results showed that each task instruction has a different effect on one or more psychomotor characteristics of the trainee, which high- lights the importance of personalized training programs.",Aliza Aliza: Kadir Has University; Irene zaugg: Colorado State University; Elif Çelik: Kadir Has University; Wolfgang Stuerzlinger: Simon Fraser University; Francisco Raul Ortega: Colorado State University; Anil Ufuk Batmaz: Concordia University; Mine Sarac: Kadir Has University,
P1243,A3RT: Attention-Aware AR Teleconferencing with Life-Size 2.5D Video Avatars,Xuanyu Wang,xwang2247-c@my.cityu.edu.hk,"In video-avatar-based multiparty AR teleconferencing, non-verbal cues indicating ""who is looking at whom"" are always lost or misdelivered. Existing solutions to the awareness of such non-verbal cues lack immersion, are less feasible for everyday usage, or lose remote users' authentic appearances. In this paper, we decompose such attention awareness into the lookee's awareness and the onlooker's awareness, and propose the ""Attention Circle"" layout and the ""rotatable 2.5D video avatar with attention thumbnail"" visualization to address them. We implement A3RT, a proof-of-concept prototype that empowers attention-aware 2.5D-video-avatar-based multiparty AR teleconferencing, and conduct evaluations to verify its effectiveness.",Xuanyu Wang: Xi'an Jiaotong University; Weizhan Zhang: Xi'an Jiaotong University; Hongbo Fu: City University of Hong Kong,
P1301,FanPad: A Fan Layout Touchpad Keyboard for Text Entry in VR,Jian Wu,lanayawj@buaa.edu.cn,"Text entry poses a significant challenge in virtual reality (VR). This paper introduces FanPad, a novel solution that facilitates dual-hand text input within head-mounted displays (HMDs). FanPad accomplishes this by ingeniously mapping and curving the QWERTY keyboard onto the touchpads of both controllers. The curved key layout of FanPad is derived from the natural movement of the thumb when interacting with the touchpad, resembling an arc with a thumb-length fixed radius.  We have conducted two comprehensive user studies to assess and evaluate the performance of our FanPad method. Notably, novices achieved a typing speed of 19.73 words per minute (WPM). The highest typing speed reached an impressive 24.19 WPM.",Jian Wu: Beihang University; Ziteng Wang: Beihang University; Lili Wang: Beihang University; Jiaheng Li: Beihang University; Yuhan Duan: Beihang University,
P1589,Exploring Augmented Reality's Role in Enhancing Spatial Perception for Building Facade Retrofit Design for Non-experts,John Sermarini,john.sermarini@ucf.edu,"This paper investigates the decision-making outcomes and cognitive-physical load implications of integrating a Building Information Modeling-driven Augmented Reality (AR) system into retrofitting design and how movement is best leveraged to understand daylighting impacts. We conducted a study with 128 non-expert participants, who were asked to choose a window facade to improve an interior space. We found no significant difference in the overall decision-making outcome between those who used an AR tool or a conventional desktop approach and that greater eye movement in AR was related to non-experts better balancing the complicated impacts facades have on daylight, aesthetics, and energy.",John Sermarini: University of Central Florida; Robert A. Michlowitz: University of Central Florida; Joseph LaViola: University of Central Florida; Lori C. Walters: University of Central Florida; Roger Azevedo: University of Central Florida; Joseph T. Kider Jr.: University of Central Florida,
P1271,Exploring Controller-based Techniques for Precise and Rapid Text Selection in Virtual Reality,Hai-Ning Liang,haining.liang@xjtlu.edu.cn,"Text selection, while common, can be difficult because the letters and words are too small and clustered together to allow precise selection. There has been limited exploration of techniques that support accurate and rapid text selection at the character, word, sentence, or paragraph levels in VR HMDs. We present three controller-based text selection methods: Joystick Movement, Depth Movement, and Wrist Orientation. They are evaluated against a baseline method via a user study with 24 participants. Results show that the three proposed techniques significantly improved the performance and user experience over the baseline, especially for the selection beyond the character level.",Jianbin Song: Xi'an Jiaotong-Liverpool University; Rongkai Shi: Xi'an Jiaotong-Liverpool University; Yue Li: Xi'an Jiaotong-Liverpool University; BoYu Gao: Jinan University; Hai-Ning Liang: Xi'an Jiaotong-Liverpool University,
P1075,Listen2Scene: Interactive material-aware binaural sound propagation for reconstructed 3D scenes,Mr Anton Jeran Ratnarajah,jeran@umd.edu,"We present an end-to-end binaural audio rendering approach (Listen2Scene) for VR and AR applications. We propose a novel neural-network-based binaural sound propagation method to generate acoustic effects for indoor 3D models of real environments. Any clean audio or dry audio can be convolved with the generated acoustic effects to render audio corresponding to the real environment. We have evaluated the accuracy of our approach with binaural acoustic effects generated using an interactive geometric sound propagation algorithm and captured real acoustic effects / real-world recordings. The demo videos, code and dataset are available online \footnote{\url{https://anton-jeran.github.io/Listen2Scene/}.","Anton Jeran Ratnarajah: University of Maryland, College Park; Dinesh Manocha: University of Maryland",
P1880,HandyNotes: using the hands to create semantic representations of contextually aware real-world objects,Aline Menin,aline.menin@inria.fr,"This paper uses Mixed Reality (MR) technologies to provide a seamless integration of digital information in physical environments through human-made annotations. Creating digital annotations of physical objects evokes many challenges for performing (simple) tasks such as adding digital notes and connecting them to real-world objects. For that, we have developed an MR system using the Microsoft HoloLens2 to create semantic representations of contextually-aware real-world objects while interacting with holographic virtual objects. User interaction is enhanced with use of fingers as placeholders for menu items. We demonstrate our approach through two real-world scenarios. We also discuss the challenges for using MR technologies.",Clément Quere: Université Côte d'Azur; Aline Menin: Université Côte d'Azur; Raphaël Julien: Université Côte d'Azur; Hui-Yin Wu: Centre Inria d'Université Côte d'Azur; Marco Winckler: Université Côte d'Azur,
P1148,Automatic Indoor Lighting Generation Driven by Human Activity Learned from Virtual Experience,Jingjing Liu,m18627293699@163.com,"A good indoor lighting solution should fit with people's habitual activity and have a low energy cost. However, it's challenging to capture and model human activity in reality due to its high complexity, let alone incorporating it into lighting planning. To solve this problem, we propose a novel framework for automatic indoor lighting generation driven by human activity learned from virtual experience. We first harnesses VR to simulate and model the user's daily activities within an indoor scene, and then devises a robust objective function which encompasses multiple activity-driven cost terms for lighting layout optimization. At last, an optimization algorithm is applied to search for the optimal lighting solution.",Jingjing Liu: Zhejiang University; Jianwen Lou: Zhejiang University; Youyi Zheng: Zhejiang University; Kun Zhou: Zhejiang University,
P1811,ConnectVR: A Trigger-Action Interface for Creating Agent-based Interactive VR Stories,Mengyu Chen,mengyuchenmat@gmail.com,"We present ConnectVR, a trigger-action interface to enable non-technical creators design agent-based narrative experiences. Our no-code authoring method specifically focuses on the design of narratives driven by a series of cause-effect relationships triggered by the player's actions. We asked 15 participants to use ConnectVR in a workshop study as well as two artists to extensively use our system to create VR narrative projects in a three-week in-depth study. Our findings shed light on the creative opportunities facilitated by ConnectVR's trigger-action approach, particularly its capability to establish chained behavioral effects between virtual agents.",Mengyu Chen: University of California Santa Barbara; Marko Peljhan: University of California Santa Barbara; Misha Sra: UCSB,
P1737,The Influence of Environmental Context on the Creation of Cartoon-like Avatars in Virtual Reality,Pauline Bimberg,bimberg@uni-trier.de,"The user study presented in this paper explores the effects that being immersed in different virtual scenes has on a user's avatar-design behavior. For this purpose, we have developed a character creation tool that lets users configure their appearance in Virtual Reality.  This tool has then been employed in a user study involving 33 participants, who were asked to configure a virtual avatar in a beach and a hospital environment. Our results show that the environment that participants were immersed in influenced their design behavior, with the beach environment leading to a more extensive use of accessories than the hospital scene.",Pauline Bimberg: University of Trier; Michael Feldmann: Trier University; Benjamin Weyers: Trier University; Daniel Zielasko: University of Trier,
P1484,DocuBits: VR Document Decomposition for Procedural Task Completion,Geonsun Lee,gsunlee@umd.edu,"DocuBits is a novel method for transforming monolithic instructional documents into small, interactive elements for VR. It enables users to create, position, and use these elements to monitor and share progress in collaborative VR learning environments. Two user studies, involving both individual and paired participants performing a chemistry lab task, demonstrated that DocuBits significantly improves usability and reduces perceived workload. In collaborative scenarios, it notably enhances social presence, collaborator awareness, and immersion. DocuBits offers valuable insights for integrating text-based instructions to support enhanced collaboration in VR settings.",Geonsun Lee: University of Maryland; Jennifer Healey: Adobe Research; Dinesh Manocha: University of Maryland,
P1128,VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games,He Zhang,hpz5211@psu.edu,"This study focuses on analyzing fear in virtual reality (VR) using horror games. We collected multi-modal data (posture, audio, physiological signals) from 23 participants to understand fear responses. Our LSTM-based model achieved prediction accuracies of 65.31% (6-level fear classification) and 90.47% (2-level fear classification). We developed the VRMNbD, a unique multi-modal dataset focusing on natural human fear responses in VR interactive environments, surpassing existing datasets in data scale, collection method, and audience scope. This research contributes to advancements in immersive game development, scene creation, and virtual human-computer interactions by providing insights into fear emotions in VR environments.",He Zhang: Tsinghua University; Xinyang Li: Tsinghua University; Yuanxi Sun: Communication University of China; Xinyi Fu: Tsinghua University; Christine Qiu: KTH Royal Institute of Technology; John M. Carroll: Pennsylvania State University,
P1392,GazePuffer :Hands-Free Input Method Leveraging Puff Cheeks for VR,Yunfei Lai,laiyf22@mails.jlu.edu.cn,"This paper introduces GazePuffer, a method that addresses the lack of a confirmation mechanism in gaze input by integrating cheek puffing with gaze. We explore the design space of cheek puffing gestures, presenting a set of nine gestures and their corresponding operations in virtual reality. We achieved a 93.8% accuracy rate in recognizing five cheek puffing gestures. In an experiment based on Fitts' Law, we compared GazePuffer's performance with two methods, demonstrating its performance is comparable to Gaze&Pinch and slightly superior to Gaze&Dwell in terms of efficiency. Finally, we showcase the practical applicability of GazePuffer in real-world VR interaction tasks.",Yunfei Lai: JiLin University; Minghui Sun: Jilin University; Zhuofeng Li: Jilin University,
P1440,ThermalGrasp: Enabling Thermal Feedback even while Grasping and Walking,Alex Mazursky,alexmazursky@uchicago.edu,"We present ThermalGrasp, a solution for wearable thermal interfaces. Unlike conventional setups that hinder users from grasping or walking by attaching Peltier elements and bulky cooling systems directly to the palm or sole, ThermalGrasp relocates these components to non-essential areas, such as the dorsal hand or foot. Our approach facilitates heat conduction to/from the palm or sole via thin, compliant materials, allowing users to interact with real objects while experiencing thermal feedback. This strikes a balance between thermal and haptic realism, enabling users to, for instance, feel the heat of a torch-like prop in virtual reality without sacrificing the ability to grasp.",Alex Mazursky: University of Chicago; Jas Brooks: University of Chicago; Beza Desta: University of Chicago ; Pedro Lopes: University of Chicago,
P1377,"Springboard, Roadblock, or ""Crutch""?: How Transgender Users Leverage Voice Changers for Gender Presentation in Social Virtual Reality",Kassie C Povinelli,kassie.povinelli@wisc.edu,"Social virtual reality (VR) serves as a vital platform for TGNC individuals to explore their identities through avatars and build online communities. However, it presents a challenge: the disconnect between avatar embodiment and voice representation, often leading to misgendering and harassment. We interviewed 13 transgender and gender-nonconforming users, finding that using a voice changer not only reduces voice-related harassment, but also allows them to experience gender euphoria through their modified voice, motivating them to pursue voice training and medication to achieve desired voices. Furthermore, we identified technical barriers and possible improvements to voice changer technology for TGNC individuals.",Kassie C Povinelli: University of Wisconsin-Madison; Yuhang Zhao: University of Wisconsin-Madison,
P2110,SafeRDW: Keep VR Users Safe When Jumping with Redirected Walking,Sen-Zhe Xu,xsz15@tsinghua.org.cn,"Existing redirected walking (RDW) algorithms typically focus on reducing collisions between users and obstacles during walking but overlook the safety when users perform significant actions such as jumping. This oversight can pose serious risks to users during VR exploration, especially when there are physical obstacles or boundaries near the virtual locations that require user jumping. We propose SafeRDW, the first RDW algorithm that takes the user's jumping safety into consideration. The proposed method can redirect users to safe physical locations when a jump is required in the virtual space, ensuring user safety. Simulation experiments and user study results both show that our method not only reduces the number of resets, but also significantly ensures user safety when they reach the jumping points in the virtual scene.",Sen-Zhe Xu: Tsinghua University; Kui Huang: Tsinghua University; Cheng-Wei Fan: Tsinghua University; Song-Hai Zhang: Tsinghua University,
P1791,Invisible Mesh: Effects of X-Ray Vision Metaphors on Depth Perception in Optical-See-Through Augmented Reality,Haoyu Tan,tan00213@umn.edu,"This paper investigates the impact of X-ray vision metaphors on distance estimation in optical-see-through augmented reality in action space. A within-subjects study was conducted to evaluate depth judgments across five conditions, including a novel ""invisible mesh"" technique. Although quantitative results regarding the impact of different X-ray vision metaphors on distance perception were inconclusive, participant feedback revealed a diversity of strategies and preferences. Overall, the findings suggest that no single metaphor was considered universally superior. This research contributes to understanding of X-ray vision techniques and informs the design considerations for AR systems aiming to enhance depth perception and user experience.",Haoyu Tan: University of Minnesota; Tongyu Nie: University of Minnesota; Evan Suma Rosenberg: University of Minnesota,
P1973,EmoFace: Audio-driven Emotional 3D Face Animation,Dr Ye Pan,whitneypanye@sjtu.edu.cn,"EmoFace is a novel audio-driven methodology for creating facial animations with vivid emotional dynamics. It has the ability to generate dynamic facial animations with diverse emotions, synchronized lip movements, and natural blinks. Incorporating independent speech and emotion encoders, our approach establishes a robust link between audio, emotion, and facial controller rigs. Post-processing techniques enhance authenticity, focusing on blinks and eye movements. We also contribute an emotional audio-visual dataset and derive control parameters for each frames to drive MetaHuman models. Quantitative assessments and user studies validate the efficacy of our innovative approach.",Chang Liu: Shanghai Jiao Tong University; Qunfen Lin: Tencent Games; Zijiao Zeng: Tencent Games; Ye Pan: Shanghai Jiaotong University,
P2072,Design and Validation of a Library of Active Affective Tasks for Emotion Elicitation in VR,Jason Wolfgang Woodworth,downtothelastpixel@gmail.com,"Emotion recognition models require datasets of physiological responses to stimuli designed to elicit targeted emotions. Many libraries of such stimuli have been created involving passive media such as images or videos. Virtual Reality, however, offers an opportunity to investigate uniquely active emotion elicitation stimuli that directly center the user in the experience. We leverage this to introduce a set of four active affective tasks in VR designed to quickly elicit targeted emotions without need for narrative understanding common to passive stimuli. We compare our tasks with selections from an existing library 360° videos and validate our approach by comparing self-reported emotional responses to the stimuli.",Jason Wolfgang Woodworth: University of Louisiana at Lafayette; Christoph W Borst: University of Louisiana at Lafayette,
P1663,The Benefits of Near-field Manipulation and Viewing to Distant Object Manipulation in VR,Jung-Hong Chuang,jhchuang@cs.nctu.edu.tw,"In this contribution, we propose to enhance two distant object manipulation techniques, BMSR and Scaled HOMER, via near-field scaled replica manipulation and viewing. In the proposed Direct BMSR, users are allowed to directly manipulate the target replica in their arm's reach space. In Scaled HOMER+NFSRV, Scaled HOMER is augmented with a near-field scaled replica view of the target object and its context. We conducted a between-subjects empirical evaluation of BMSR, Direct BMSR, Scaled HOMER, and Scaled HOMER+NFSRV. Our findings revealed that Direct BMSR and Scaled HOMER+NFSRV significantly outperformed BMSR and Scaled HOMER, respectively, in terms of accuracy.",Sabarish V. Babu: Clemson University; Wei-An Hsieh: National Yang Ming Chiao Tung University; Jung-Hong Chuang: National Yang Ming Chiao Tung University,
P1858,The Impact of Avatar Stylization on Trust,Michael Neff,michael.neff@gmail.com,"Virtual Reality (VR) allows people to choose any avatar to represent themselves. How does this choice impact social interaction that often relies on the establishment of trust? Are people more likely to trust a highly realistic avatar or is there flexibility in representation? This work presents a study exploring this question using a high stakes medical scenario. Participants meet three different doctors with three different style levels: realistic, caricatured, and an in-between ``Mid'' level. Trust ratings are largely consistent across style levels, but participants were more likely to select doctors with the ``Mid'' level of stylization for a second opinion. There is a clear preference against one of the three doctor identities.","Ryan Canales: Reality Labs Research, Meta; Doug Roble: Reality Labs Research, Meta; Michael Neff: Reality Labs Research, Meta",
P1518,Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters,Zechen Bai,ustbbzch@gmail.com,"This paper introduces a comprehensive approach to automatically generate facial animations for customized characters, irrespective of their blendshape topologies and texture appearances. The method involves estimating blendshape coefficients from input images or videos using a deep learning model. The proposed toolkit incorporates this model, featuring user-friendly interfaces and a human-in-the-loop scheme. Evaluation results demonstrate the flexibility to support personalized virtual character models. The toolkit facilitates easy and efficient facial animation generation, yielding satisfactory quality. Human-in-the-loop involvement enhances solution performance.","Zechen Bai: National University of Singapore; Peng Chen: Institute of Software, Chinese Academy of Sciences; Xiaolan Peng: Institute of software,Chinese Academy of Sciences; Lu Liu: Institute of Software, Chinese Academy of Sciences; Naiming Yao: Institute of Software, Chinese Academy of Sciences; Hui Chen: Institute of Software, Chinese Academy of Sciences",
P1936,SkiMR: Dwell-free Eye Typing in Mixed Reality,Ms Jinghui Hu,jh2265@cam.ac.uk,"We present SkiMR: a dwell-free eye typing system that enables fast and accurate hands-free text entry on mixed reality headsets. SkiMR uses a statistical decoder to infer users' intended text based on users' eye movements on a virtual keyboard, bypassing the need for dwell timeouts. We conducted two studies with a HoloLens 2: the first (n=12) showed SkiMR's superiority in speed over traditional dwell-based and a hybrid method. The second study (n=16) focused on composition tasks with a refined system, revealing that users could compose text at 12 words per minute with a 1.1% corrected error rate. Overall, this work demonstrates the high potential for fast and accurate hands-free text entry for MR headsets.",Jinghui Hu: University of Cambridge; John J Dudley: University of Cambridge; Per Ola Kristensson: University of Cambridge,
P1006,"Asynchronously Assigning, Monitoring, and Managing Assembly Goals in Virtual Reality for High-Level Robot Teleoperation",Jen-Shuo Liu,jl5004@columbia.edu,"We present a prototype virtual reality user interface for robot teleoperation that supports high-level goal specification in remote assembly tasks. Users interact with virtual replicas of task objects. They asynchronously assign multiple goals in the form of 6DoF destination poses without needing to be familiar with specific robots and their capabilities, and manage and monitor the execution of these goals. The user interface employs two different spatiotemporal visualizations for assigned goals: one represents all goals within the user’s workspace, while the other depicts each goal within a separate world in miniature. We conducted a user study of the interface without the robot system to compare these visualizations.",Shutaro Aoyama: Columbia University; Jen-Shuo Liu: Columbia University; Portia Wang: Columbia University; Shreeya Jain: Columbia University; Xuezhen Wang: Columbia University; Jingxi Xu: Columbia University; Shuran Song: Columbia University; Barbara Tversky: Columbia Teachers College; Steven Feiner: Columbia University,
P1135,A Novel Approach for Virtual Locomotion Gesture Classification: Self-Teaching Vision Transformer for a Carpet-Type Tactile Sensor,Sung-Ha Lee,shlee0414@gm.gist.ac.kr,"Locomotion gesture classification in virtual reality is the process of analyzing and identifying specific user movements in the real world to navigate virtual environments. In this paper, we utilize a high-resolution carpet-type tactile sensor as a foot action recognition interface. This interface can capture the user's foot pressure data in detail to distinguish similar actions. To efficiently process the captured user's detailed foot tactile data and classify nuanced actions, we propose a novel self-teaching vision transformer (STViT) model integrating elements of the shifted window vision transformer (SwinViT) and data-efficient image transformer (DeiT). However, unlike DeiT, our model uses itself from N-steps prior as the teacher model.",Sung-Ha Lee: Gwangju Institute of Science and Technology; Ho-Taek Joo: (GIST) Gwangju Institute of Science and Technology; Insik Chung: School of Integrated Technology; Donghyeok Park: None; Yunho Choi: Gwang-ju institute of science and technology; KyungJoong Kim: GIST,
P1712,Exploring the Role of Expected Collision Feedback in Crowded Virtual Environments,Haoran Yun,haoran.yun@upc.edu,"This study explores how expected collision feedback affects user interaction and perception in virtual environments populated by dynamic virtual humans. While real-world locomotion is influenced by collision risks, such risks are often absent in virtual settings. The paper examines the effectiveness of auditory cues, tactile vibrations, and the expectation of physical bumps in enhancing collision realism in virtual crowds. Results show that anticipated collision risk significantly alters participant behavior—both in overall navigation and local movement—and strengthens the sense of presence and copresence. Auditory cues notably impact navigation and copresence, whereas tactile feedback primarily affects local movements.",Haoran Yun: Universitat Politècnica de Catalunya; Jose Luis Ponton: Universitat Politècnica de Catalunya; Alejandro Beacco: Universitat Politècnica de Catalunya; Carlos Andujar: Universitat Politècnica de Catalunya; Nuria Pelechano: Universitat Politècnica de Catalunya,
P1106,Understanding Interaction and Breakouts of Safety Boundaries in Virtual Reality Through Mixed-Method Studies,Wen-Jie Tseng,wenjietseng@gmail.com,"Virtual Reality (VR) technologies facilitate immersive experiences within domestic settings. Despite the prevalence of safety boundaries implemented by commercial VR products to mitigate collisions, empirical insights into how people perceive and interact with these safety features remain underexplored. This paper presents two mixed-method design studies (an online survey and a lab study) to investigate attitudes, behavior, and reasons when interacting with safety boundaries. Our empirical findings reveal VR participants sometimes break out of safety boundaries based on their real-world spatial information. Finally, we discuss improving future VR safety mechanisms by supporting participants' real-world spatial information using landmarks.","Wen-Jie Tseng: Technical University of Darmstadt; Petros Dimitrios Kontrazis: Telecom paris; Eric Lecolinet: Institut Polytechnique de Paris; Samuel Huron: Télécom Paris, Institut Polytechnique de Paris; Jan Gugenheimer: TU-Darmstadt",
P1457,Divide-Conquer-and-Merge: Memory- and Time-Efficient Holographic Displays,Jidong Jia,jjd1123@sjtu.edu.cn,"We proposed a divide-conquer-and-merge strategy to address the memory and computational capacity scarcity in ultra-high-definition CGH generation. By integrating our strategy into existing SOTA methods, HoloNet and CCNNs, we achieved significant reductions in GPU memory usage during the training period by 64.3% and 12.9%, respectively. Furthermore, we observed substantial speed improvements in hologram generation, with an acceleration of up to 3× and 2×, respectively. Particularly, we successfully trained and inferred 8K definition holograms on an NVIDIA GeForce RTX 3090 GPU for the first time in simulations. Furthermore, we conducted full-color optical experiments to verify the effectiveness of our method.",Zhenxing Dong: Shanghai Jiao Tong University; Jidong Jia: Shanghai Jiao Tong University; Yan Li: Shanghai Jiao Tong University; Yuye Ling: Shanghai Jiao Tong University,
P1781,Beyond Looks: A Study on Agent Movement and Audiovisual Spatial Coherence in Augmented Reality,Stephanie Arevalo Arboleda,stephanie.arevalo@tu-ilmenau.de,"The interplay of virtual humans' rendering style, movements, and associated sounds in real-world interactions, particularly in Augmented Reality, is yet to be explored. In this paper, we investigate the influence of three distinct movement patterns (circle, diagonal, and standing), two rendering styles (realistic and cartoon), and two types of audio (spatial audio and non-spatial audio) on emotional responses, social presence, appearance and behavior plausibility, audiovisual coherence and auditory plausibility in a study (N=36) where participants observed an agent reciting a short story. Our results indicate movement and rendering style influence the collected measures and point towards a minor effect of audio rendering.",Stephanie Arevalo Arboleda: Ilmenau University of Technology; Christian Kunert: Technische Universität Ilmenau; Jakob Hartbrich: Technische Universität Ilmenau; Christian Schneiderwind: Technische Universität Ilmenau; Chenyao Diao: Technische Universität Ilmenau; Christoph Gerhardt: Technische Universität Ilmenau; Tatiana Surdu: Technische Universität Ilmenau; Florian Weidner: Lancaster University; Wolfgang Broll: Ilmenau University of Technology; Stephan Werner: Technische Universität Ilmenau; Alexander Raake: Technische Universität Ilmenau,
P1136,GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance,Shiyu Li,shiyu.li@fau.de,"Guidance for assemblable parts is a promising field for the use of augmented reality. Augmented reality assembly guidance requires 6D object poses of target objects in real-time. To address this problem, we present Graph-based Object Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The real-time markerless multi-object tracking is initialized via 6D pose estimation and updates the graph-based assembly poses. By utilizing relative poses of the individual assembly parts, we update the multi-state assembly graph. Quantitative experiments in synthetic data and further qualitative study in real test data show that GBOT can outperform existing work towards enabling context-aware augmented reality assembly guidance.",Shiyu Li: Technical University of Munich; Hannah Schieber: Friedrich-Alexander University; Niklas Corell: Friedrich-Alexander University Erlangen-Nürnberg; Bernhard Egger: Friedrich-Alexander-Universität Erlangen-Nürnberg; Julian Kreimeier: Technical University of Munich; Daniel Roth: Technical University of Munich,
P1851,Supporting Text Entry in Virtual Reality with Large Language Models,Liuqing Chen,chenlq@zju.edu.cn,"This study investigates improving text entry efficiency in virtual reality by utilizing large language models. It introduces three LLM-assisted methods: Simplified Spelling, Content Prediction, and Keyword-to-Sentence Generation, aligning with user cognition and English text predictability at various levels. Tested on a VR prototype, these methods significantly reduce manual keystrokes by 16.4% to 49.9%, boosting efficiency by 21.4% to 76.3%. These methods lessen task loads without increasing manual corrections, enhancing overall usability. Long-term observations show that user proficiency with these methods further enhances text entry efficiency.",Liuqing Chen: Zhejiang University; Yu Cai: Zhejiang University; Ruyue Wang: Zhejiang University; Shixian Ding: Zhejiang University; Yilin Tang: Zhejiang University; Preben Hansen: Stockholm University; Lingyun Sun: Zhejiang University,
P1120,Foveated Fluid Animation in Virtual Reality,Yue Wang,wang_yue@sjtu.edu.cn,"In this paper, we propose a novel foveated fluid simulation method that optimizes physics-based fluid simulation by leveraging the human visual system's natural foveation for VR. Dividing the simulation into foveal, peripheral, and boundary regions, our system dynamically allocates computational resources, balancing accuracy and efficiency. Implemented with a multi-scale approach, our method achieves a 2.27x speedup while maintaining perceptual quality. Subjective studies validate its effectiveness, and we explore the impact of metrics like particle radius and viewing distance on visual effects. Our work pioneers real-time foveated fluid simulation in VR, advancing efficiency and realism for fluid-based VR applications.",Yue Wang: Shanghai Jiao Tong University; Yan Zhang: Shanghai Jiao Tong University; Xuanhui Yang: Shanghai Jiao Tong University; Hui Wang: Shanghai Jiao Tong University; Dongxu Liu: Shanghai Jiao Tong University; Xubo Yang: SHANGHAI JIAO TONG UNIVERSITY,
P1189,Selection Performance and Reliability of Eye and Head Gaze Tracking Under Varying Light Conditions,Alexander Marquardt,alexander.marquardt@h-brs.de,"In this paper, we present two studies exploring the influence of environmental factors, particularly lighting conditions and spatial properties, on eye and head gaze tracking in Augmented Reality (AR). Our findings show that eye tracking tends to be faster but faces inconsistencies, particularly under dynamic lighting conditions. In contrast, head gaze tracking, though more consistent, experiences reduced accuracy in environments with fluctuating light levels. Additionally, the spatial properties of the environment significantly influence both tracking methods. Our findings emphasize the importance of these factors in choosing a tracking method and the need for AR systems to adapt dynamically to environmental changes.",Alexander Marquardt: Institute of Visual Computing; Melissa Steininger: Institute of Visual Computing; Christina Trepkowski: Institute of Visual Computing; Martin Weier: RheinMain University of Applied Sciences; Ernst Kruijff: Bonn-Rhein-Sieg University of Applied Sciences,
P1247,ACHOO - Bless you! Sense of Presence can provoke Proactive Mucosal Immune Responses in Immersive Human-Agent Interactions,Prof. Dr. Frank Steinicke,frank.steinicke@uni-hamburg.de,"Previous work suggests that the mere visual perception of disease cues can proactively enhance mucosal immune responses even without actual pathogen exposure. We present the first immersive immunological experiment, which investigates if social interactions with virtual agents in VR can lead to a mucosal immune response, in particular, a proactive release of secretory immunoglobin A (sIgA) in saliva. Therefore, we simulated a virtual bus stop scenario in which participants were required to closely approach and establish eye contact with ten agents. We found that sIgA secretion increased when agents sneezed as well as when they did not sneeze. In the latter, the increase was correlated with the perceived involvement and sense of presence.",Judith Katharina Keller: Universität Hamburg; Agon Kusari: Universität Hamburg; Sophie Czok: Universität Hamburg; Birgit Simgen: MVZ Volkmann Laboratory; Frank Steinicke: Universität Hamburg; Esther Diekhof: Universität Hamburg,
P1223,Understanding the Impact of Longitudinal VR Training on Users with Mild Cognitive Impairment Using fNIRS and Behavioral Data,Prof. Lingguo Bu,bulingguo@sdu.edu.cn,"This study presents a VR-based cognitive training system for mild cognitive impairment (MCI) rehabilitation. It leverages multi-modal data to assess longitudinal training impact using brain activation, network connectivity, behavior indicators, and MoCA scores. A two-month experiment validates the feedback methodologies and explores training duration effects on rehabilitation efficacy. Results indicate significant positive outcomes for MCI rehabilitation and inform VR system refinement. The research highlights the importance of quantitative, longitudinal assessments in rehabilitation efficacy studies.",Jing Qu: Shandong University; Shantong Zhu: Shandong University; Yiran Shen: Shandong University; Yanjie Zhang: The Hong Kong Polytechnic University; Lingguo Bu: Shandong University,
P1685,DreamCodeVR: Towards Democratizing Behavior Design in Virtual Reality with Speech-Driven Programming,Daniele Giunchi,d.giunchi@ucl.ac.uk,"VR content creation remains a complex and challenging task, requiring specialized skills and knowledge. Powered by large language models (LLMs), DreamCodeVR is designed to assist users, irrespective of their coding skills, in crafting basic object behavior in VR environments by translating spoken language into code within an active application. Our preliminary user study indicated that the system can support elementary programming tasks. However, it also uncovered a wide range of challenges and areas for future research, which we detail in an extensive discussion.",Daniele Giunchi: University College London; Nels Numan: University College London; Elia Gatti: University College London; Anthony Steed: University College London,
P1264,Presentation of Finger-size Shapes by Combining Force Feedback and Electro-tactile Stimulation,Yui Suga,suga@kaji-lab.jp,"Perceiving shapes quickly and accurately is vital for effective interaction and immersion in VR. Force feedback devices often fail to convey precise geometric details like edges. We enhanced these devices with a compact, high-density cutaneous electrical stimulation mechanism to better represent subtle shapes. Our evaluation involved shape discrimination tests on four 7.5 mm thick column types, using reactive force and edge cues. We compared conditions using only force feedback, only cutaneous feedback, and a combination of both. The results showed that the combined approach significantly improved shape discrimination accuracy, leading to a more precise perception of fine geometric details.",Yui Suga: The University of Electro-Communications; Izumi Mizoguchi: The University of Electro-Communications; Hiroyuki Kajimoto: The University of Electro-Communications,
P1212,Understanding Online Education in Metaverse: Systems and User Experience Perspectives,Ruizhi Cheng,rcheng4@gmu.edu,"Virtual reality (VR) is becoming increasingly popular in online education. However, there is currently no in-depth investigation of the user experience of VR-based online education and the comparison of it with video-conferencing-based counterparts. To fill these critical gaps, we conduct multiple sessions of two courses in a university with 10 and 37 participants on Mozilla Hubs (Hubs for short), a social VR platform, and let them compare the classroom experience on Hubs with Zoom, a popular video-conferencing application. Besides employing traditional analytical methods to understand user experience, we benefit from a measurement study of Hubs to corroborate our findings and systematically detect its performance bottlenecks.",Ruizhi Cheng: George Mason University; Erdem Murat: George Mason University; Lap-Fai Yu: George Mason University; Songqing Chen: George Mason University; Bo Han: George Mason University,
P1011,HyperXRC: Hybrid In-Person + Remote Extended Reality Classroom - A Design Study,Siyu Huang,huan1882@purdue.edu,"This paper investigates HyperXRC, a hybrid classroom design that accommodates both local and remote students. The instructor wears an extended reality (XR) headset that shows the local classroom and the local students, as well as remote students modeled with live video sprites placed in empty classroom seats. A controlled user study reveals that instructors are more likely to detect remote student actions in the HyperXRC condition (59%) than in a conventional videoconferencing condition (36%), and that local students prefer the lecture when the instructor does not wear the XR headset.",Siyu Huang: Purdue University; Voicu Popescu: Purdue University,
P1020,Evaluating Transitive Perceptual Effects Between Virtual Entities in Outdoor Augmented Reality,Juanita Benjamin,juanita.benjamin@ucf.edu,"One of the most significant issues with augmented reality display systems is the perception of computer-generated content, which differs from human perception of real-world objects or entities. This can lead to distrust in these systems, which can have negative consequences for various applications such as distance and size judgments. We present a study that investigates how individuals perceive size, distance, and speed on 3D augmented reality objects in an outdoor setting. In this experiment, participants made estimated judgments about two types of entities: unfamiliar and familiar. Distance and speed were found to be underestimated, whereas size estimates were close to accurate.","Juanita Benjamin: University of Central Florida; Austin Erickson: KBR, Air Force Research Lab; Matt Gottsacker: University of Central Florida; Gerd Bruder: University of Central Florida; Greg Welch: University of Central Florida",
P1294,Avatar360: Emulating 6-DoF Perception in 360° Images through Avatar-Assisted Navigation,Andrew Chalmers,andrew.chalmers@vuw.ac.nz,"360° images provide a panoramic view into captured environments, placing users within an egocentric perspective. However, users are not able to experience 6-DoF navigation with translational movement. We propose Avatar360, a novel method to elicit 6-DoF perception in 360° panoramas. We seamlessly integrate a 3D avatar into the 360° panoramas, allowing users to navigate a 3D virtual landscape congruent with the 360° background. By aligning the exocentric perspective of the 360° panorama with the avatar's movements, we successfully replicate a sensation of 6-DoF navigation within 360° panoramas. A user study was conducted, showing that avatar-assisted navigation can convincingly elicit a user's sensation of movement within 360° panoramas.",Andrew Chalmers: Victoria University of Wellington; Faisal Zaman: Victoria University of Wellington; Taehyun James Rhee: Victoria University of Wellington,
P1098,On the Emergence of Symmetrical Reality,Zhenliang Zhang,zzlyw10@gmail.com,"In this paper, we introduce the symmetrical reality framework, which offers a unified representation encompassing various forms of physical-virtual amalgamations. This framework enables researchers to better comprehend how AI agents can collaborate with humans and how distinct technical pathways of physical-virtual integration can be consolidated from a broader perspective. We then delve into the coexistence of humans and AI, demonstrating a prototype system that exemplifies the operation of symmetrical reality systems for specific tasks, such as pouring water. Finally, we propose an instance of an AI-driven active assistance service that illustrates the potential applications of symmetrical reality.",Zhenliang Zhang: Beijing Institute for General Artificial Intelligence; Zeyu Zhang: Beijing Institute for General Artificial Intelligence; Ziyuan Jiao: Beijing Institute for General Artificial Intelligence; Yao Su: Beijing Institute for General Artificial Intelligence; Hangxin Liu: Beijing Institute for General Artificial Intelligence; Wei Wang: Beijing Institute for General Artificial Intelligence; Song-Chun Zhu: Beijing Institute for General Artificial Intelligence,
P1172,DreamSpace: Dreaming Your Room Space with Text-Driven Holistic Texture Propagation,Bangbang Yang,ybbbbt@gmail.com,"DreamSpace allows users to personalize their own spaces' appearances with text prompts and delivers immersive VR experiences on HMD devices. Specifically, given a real-world captured room, we generate enchanting and holistic mesh textures based on the user's textual inputs, while ensuring semantic consistency and spatial coherence, such as the sofa still retains its recognizable form as a sofa, but in fantasy styles.","Bangbang Yang: Bytedance Inc; Wenqi Dong: Zhejiang University; Lin Ma: PICO MixedReality, Bytedance Inc; Wenbo Hu: PICO MixedReality, Bytedance Inc; Xiao Liu: PICO MixedReality, Bytedance Inc; Zhaopeng Cui: Zhejiang University; Yuewen Ma: PICO MixedReality, Bytedance Inc",
P1810,Force-regulated Elastic Linear Objects Tracking for Virtual and Augmented Reality,Lifeng Zhu,lfzhulf@gmail.com,"We present a method for interactive tracking of elastic rods, which are commonly encountered in daily life. Although humans are sensitive to shape changes of rods, estimating external forces responsible for the deformation is not intuitive. We develop a particle filter-based framework and employ the Cosserat rod model to regulate noisy points, turning the inverse physics problem into a forward simulation and search problem. The method can simultaneously digitalize shapes and external forces on real-world elastic rods. We demonstrate virtual and augmented reality applications to facilitate the interaction with elastic linear objects. The tracking performance is also validated with experiments.",Yusheng Luo: Southeast University; Lifeng Zhu: Southeast University; Aiguo Song: Southeast University,
P1762,Impact of multimodal instructions for tool manipulation skills on performance and user experience in an Immersive Environment,Dr. Amine Chellali,amine.chellali@univ-evry.fr,"This study explores the use of multimodal communication to convey instructions to learners on the amplitude of movements to perform in an immersive environment. The study aims to examine the impact of four modality combinations on performance, workload, and user experience. The results show that participants achieved higher accuracy with the visual-haptic and verbal-visual-haptic conditions. Moreover, they performed the movements faster, and their movement trajectories were closer to the reference trajectories in the visual-haptic condition. Finally, the most preferred verbal-visual-haptic combination enhanced the sense of presence, co-presence, social presence, and learning experience. No impact on workload was observed.","Cassandre Simon: Univ Evry,Université Paris Saclay; Manel Boukli Hacene: Univ Evry, Université Paris Saclay; Flavien Lebrun: Univ Evry, Université Paris Saclay; Samir Otmane: Univ Evry, Université Paris Saclay; Amine Chellali: Univ Evry, Université Paris Saclay",
P1153,EyeShadows: Peripheral Virtual Copies for Rapid Gaze Selection and Interaction,Dr. Jason Orlosky,jasonorlosky@gmail.com,"In this paper, we present EyeShadows, an eye gaze-based selection system that takes advantage of peripheral copies of items that allow for quick selection and manipulation of an object or corresponding menus. This method is compatible with a variety of different selection tasks and controllable items, avoids the Midas touch problem, does not clutter the virtual environment, and is context sensitive. We have implemented and refined this selection tool for VR and AR, including testing with optical and video see-through displays. We demonstrate that EyeShadows can also be used for a wide range of AR and VR applications, including manipulation of sliders or analog elements.",Jason Orlosky: Augusta University; Chang Liu: Kyoto University Hospital; Kenya Sakamoto: Osaka University; Ludwig Sidenmark: University of Toronto; Adam Mansour: Augusta University,
P1669,An Empirical Evaluation of the Calibration of Auditory Distance Perception under Different Levels of Virtual Environment Visibilities,Wen-Chieh Lin,wclin@cs.nctu.edu.tw,"We investigated if perceptual learning through carryover effects of calibration occurs in different levels of a virtual environment’s visibility. Users performed an auditory depth judgment task over several trials in which they walked where they perceived an aural sound to be. This task was sequentially performed in the pretest, calibration, and posttest phases. Feedback on the perceptual accuracy of distance estimates was only provided in the calibration phase. We found that auditory depth estimates, obtained using an absolute measure, can be calibrated to become more accurate and that environments visible enough to reveal their extent may contain visual information that users attune to in scaling aurally perceived depth.",Wan-Yi Lin: National Yang Ming Chiao Tung University; Rohith Venkatakrishnan: University of Florida; Roshan Venkatakrishnan: University of Florida; Sabarish V. Babu: Clemson University; Christopher Pagano: Clemson University; Wen-Chieh Lin: National Yang Ming Chiao Tung University,
P1662,Text2VRScene: Exploring the Paradigm of Automated Generation System for VR Experience From the Ground Up,Zhizhuo Yin,zyin190@connect.hkust-gz.edu.cn,This paper explores how to generate VR scenes from text by incorporating LLMs and various generative models into an automated system. This paper first identifies the possible limitations of LLMs for an automated system and proposes a systematic framework to mitigate them. A VR scene generation system named Text2VRScene is developed based on the proposed framework with well-designed prompts. A series of tests are carried out to validate the effectiveness of this system. The results show that the framework contributes to improving the reliability of the system and the quality of the generated VR scenes and illustrate the promising performance of the system in generating satisfying VR scenes with a clear theme.,Zhizhuo Yin: Hong Kong University of Science and Technology (Guangzhou); Yuyang Wang: Hong Kong University of Science and Technology; Theodoros Papatheodorou: Hong Kong University of Science and Technology (Guangzhou); Pan Hui: The Hong Kong University of Science and Technology,
P1359,Reflecting on Excellence: VR Simulation for Learning Indirect Vision in Complex Bi-Manual Tasks,Maximilian Kaluschke,mxkl@cs.uni-bremen.de,"This study explores the efficacy of VR simulators in training dental surgery skills involving indirect vision through a mirror. To assess learning outcomes, a simulator tracked eye gaze and tool trajectories during the root canal access opening stage. Thirty fifth-year dental students underwent six training sessions, showing significant improvements in drilling on realistic plastic teeth. Students with better simulator performance demonstrated enhanced real-world test results. Eye tracking revealed correlations between correct mirror placement, continuous fixation on the tooth, and improved skills. The findings suggest that eye tracking offers valuable insights into bi-manual psychomotor skill development in indirect vision scenarios.",Maximilian Kaluschke: University of Bremen; Rene Weller: University of Bremen; Myat Su Yin: Faculty of Information and Communication Techology; Benedikt Werner Hosp: University Clinics Tübingen; Farin Kulapichitr: University of Bremen; Siriwan Suebnukarn: Thammasat University; Peter Haddawy: Mahidol University; Gabriel Zachmann: University of Bremen,
P1181,Task-based methodology to characterise immersive user experience with multivariate data,Florent Robert,florent.robert@etu.unice.fr,"Virtual Reality technologies is promising for research, however, the evaluation of the user experience in immersive environments is daunting, the richness of the media presents challenges to synchronise context with behavioural metrics.  We propose a task-based methodology that provides fine-grained descriptions and analyses of the experiential user experience in VR that (1) aligns low-level tasks with behavioural metrics, (2) defines performance components with baseline values to evaluate task performance, and (3) characterise task performance with multivariate user behaviour data. We find that the methodology allows us to better observe the experiential user experience by highlighting relations between user behaviour and task performance.",Florent Robert: Université Côte d'Azur; Hui-Yin Wu: Centre Inria d'Université Côte d'Azur; Lucile Sassatelli: Université Côte d'Azur; Marco Winckler: Université Côte d'Azur,
P1300,Generating Virtual Reality Stroke Gesture Data from Out-of-Distribution Desktop Stroke Gesture Data,Linping Yuan,atuanylp@gmail.com,"This paper exploits ubiquitous desktop strokes as an input source for generating VR stroke gestures. The commonalities between desktop and VR strokes allow the generation of additional dimensions in VR strokes. However, distribution shifts not only exist between desktop and VR datasets but also within each dataset. To build models capable of generalizing to unseen distributions, we propose a novel architecture based on cGANs, with the generator encompassing three steps: discretizing the output space, characterizing latent distributions, and learning conditional domain-invariant representations. We illustrate the applicability of the enriched VR datasets through two applications: VR stroke classification and stroke prediction.",Linping Yuan: The Hong Kong University of Science and Technology; Boyu Li: Hong Kong University of Science and Technology (GuangZhou); Jindong Wang: Microsoft Research Asia; Huamin Qu: The Hong Kong University of Science and Technology; Wei Zeng: The Hong Kong University of Science and Technology (Guangzhou),
P2082,Comparing Physics-based Hand Interaction in Virtual Reality: Custom Soft Body Simulation vs. Off-the-Shelf Integrated Solution,Christos Lougiakis,chrislou@di.uoa.gr,"Physics-based hand interaction in VR has been explored extensively, but most solutions lack usability. CLAP stands out as a custom soft body simulation offering realistic hand interaction in VR. Despite CLAP's advantages, it imposes constraints. Introducing HPTK+, a software utilizing NVIDIA PhysX, aiming for free hand interactions in virtual environments. A user study with 27 participants compared both libraries, showing a preference for CLAP. However, no significant differences in other measures or performance, except variance, were observed. These findings offer insights into library suitability for specific tasks. HPTK+'s exclusive support for diverse interactions positions it for further research in physics-based VR interactions.",Christos Lougiakis: National and Kapodistrian University of Athens; Jorge Juan González: PresenceXR; Giorgos Ganias: National and Kapodistrian University of Athens; Akrivi Katifori: Athena Research Center; Ioannis Panagiotis Ioannidis: Athena Research Center; Maria Roussou: National and Kapodistrian University of Athens,
P1602,Exploring Bi-Manual Teleportation in Virtual Reality,A K M Amanat Ullah,amanat7@student.ubc.ca,"Enhanced hand tracking in modern VR headsets has popularized hands-only teleportation that allows instantaneous movement within VR environments. However, previous works on hands-only teleportation have not fully explored - the potential of bi-manual input (where each hand plays a distinct role), the influence of users’ posture (whether sitting or standing), or assessments based on human motor models (such as Fitts’ Law). To address these gaps, we conducted a user study (N=20) to compare the performance of bi-manual and uni-manual techniques in VR teleportation tasks using a proposed Fitts’ Law model, considering both the postures. Results showed that bi-manual techniques enable faster teleportation and are more accurate than other methods.","Siddhanth Raja Sindhupathiraja: Indian Institute of Technology Delhi; A K M Amanat Ullah: University of British Columbia, Okanagan; William Delamare: ESTIA; Khalad Hasan: University of British Columbia",
P1821,A User Study on Sharing Physiological cues in VR Assembly tasks,Prasanth Sasikumar,prasanth.sasikumar.psk@gmail.com,"Understanding your partner's emotions is crucial for success in collaborative settings, particularly in remote collaboration. Our solution involves a visual representation enabling the inference of emotional patterns from physiological data, potentially influencing communication styles. A study explored the impact of this visual representation in remote collaboration, revealing minimal variation in workload linked to observing physiological cues. However, participants predominantly favored monitoring their partner's attentional state. Results indicate that most participants allocated only a small portion of their attention to partner's physiological cues, often unsure how to interpret and use the obtained information.","Prasanth Sasikumar: National University of Singapore; Ryo Hajika: The University of Auckland; Tamil Selvan Gunasekaran: The University of Auckland; Kunal Gupta: The University of Auckland; Yun Suen Pai: Keio University Graduate School of Media Design; Huidong Bai: The University of Auckland; Suranga Nanayakkara: Department of Information Systems and Analytics, National University of Singapore; Mark Billinghurst: University of South Australia",
P1698,Exploring Depth-based Perception Conflicts in Virtual Reality through Error-Related Potentials,Haolin Gao,1571438524@qq.com,"Previous research has indicated that human’s perception of depth in virtual reality (VR) differs from the real world. In this paper, we investigated users’ depth perception conflicts at varying depths within near-field in VR using Error-Related Potentials (ErrPs) from electroencephalography (EEG) data. We employed a 3 (target depth) × 2 (conflict or non-conflict) experimental design, mimicking depth perception conflicts within the near-field. We collected questionnaire results, performance data, and EEG data from users. Our results showed that a significant effect of target depth on conflict perception, and the amplitude of an ErrPs component increases with the degree of depth perception conflict.","Haolin Gao: Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Photonics, Beijing Institute of Technology; Kang Yue: Institute of Software, Chinese Academy of Sciences; Songyue Yang: Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Photonics, Beijing Institute of Technology; Yu Liu: Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Photonics, Beijing Institute of Technology; Mei Guo: Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Photonics, Beijing Institute of Technology; Yue Liu: Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Photonics, Beijing Institute of Technology",
P1898,Eyes on the Task: Gaze Analysis of Situated Visualization for Collaborative Tasks,Nelusa Pathmanathan,nelusa.pathmanathan@visus.uni-stuttgart.de,"The use of augmented reality technology to support humans within complex and collaborative tasks has gained increasing importance. Analyzing collaboration patterns is usually done by conducting observations and interviews. We argue that eye tracking can be used to extract further insights and quantify behavior. To this end, we contribute a study that uses eye tracking to investigate participant strategies for solving collaborative sorting and assembly tasks. We compare participants' visual attention during situated instructions in AR and traditional paper-based instructions as a baseline. By investigating the performance and gaze behavior of the participants, different strategies for solving the provided tasks are revealed.",Nelusa Pathmanathan: University of Stuttgart; Tobias Rau: University of Stuttgart; Xiliu Yang: Institute of Computational Design and Construction; Aimée Sousa Calepso: University of Stuttgart; Felix Amtsberg: Institute of Computational Design and Construction; Achim Menges: University of Stuttgart; Michael Sedlmair: University of Stuttgart; Kuno Kurzhals: University of Stuttgart,
P2081,Object Cluster Registration of Dissimilar Rooms Using Geometric Spatial Affordance Graph to Generate Shared Virtual Spaces,Seonji Kim,seonji.kim@kaist.ac.kr,"We propose Object Cluster Registration (OCR) using a Geometric Spatial Affordance Graph to support user interaction with multiple objects in a shared space generated from two dissimilar rooms. Previous research on generating a shared virtual space has only considered objects individually and aimed at maximizing the area, leading to limited interactions and neglecting usability. The proposed method extracts optimal object cluster pairs to align dissimilar rooms in generating shared space. The evaluation showed that object correlation preservation was higher when OCR was used with no significant difference in the area size. This suggests that factoring in the relationship between objects does not compromise the objective of maximizing area.",Seonji Kim: KAIST; Dooyoung Kim: KAIST; Jae-eun Shin: KAIST; Woontack Woo: KAIST,
P1255,A Comparative Usability Study of Physical Multi-touch versus Virtual Desktop-Based Spherical Interfaces,Nikita Soni,nnsoni@uic.edu,"Physical multi-touch spherical displays offer a hands-on experience for visualizing global data but are often too costly for learning institutions. As an alternative, virtual spherical interfaces like Google Earth have been used, but it's unclear if they offer a similar usability experience. We ran a within-subjects study with 21 users to compare usability experiences of physical and virtual spherical interfaces. Our results showed no significant difference in usability or task completion time between two platforms. Participants noted differences in effort and motor demand. Our research implies that, in resource-constrained settings, a virtual globe can be a viable alternative for a physical sphere from a usability perspective.",Nikita Soni: University of Illinois Chicago; Oluwatomisin Obajemu: University of Florida; Katarina Jurczyk: University of Florida; Chaitra Peddireddy: University of Florida ; Maeson Vallee: University of Florida; Ailish Tierney: University of Florida; Niloufar Saririan: University of Florida; Cameron John Zuck: University of Florida; Kathryn A. Stofer: University of Florida; Lisa Anthony: University of Florida,
P1621,Prototyping of Augmented Reality interfaces for air traffic alert and their evaluation using a Virtual Reality air-proximity simulator,Jose Pascual Molina Masso,jpmolina@dsi.uclm.es,"Mid-air collisions can occur, especially near airfields and in non-controlled airspaces. Technology typically helps pilots in the form of a flat display in the cockpit. AR could reach light and sport aviation in the next years thanks to new light and affordable AR glasses. In this work we rely on VR to prototype and test different AR interfaces for air traffic alert. Firstly, we proposed and tested four different HMD AR interfaces with 4 pilots in our own VR aircraft-proximity simulator. Then, the two best-scored interfaces were selected for a second evaluation, compared against Circular HUD (Alce et al.) and a fixed-mounted (FM) conventional HUD radar, tested by 4 additional pilots. Overall, pilots showed preference for our AR proposals.",Angel Torres del Alamo: University of Castilla-La Mancha; Jose Pascual Molina Masso: University of Castilla-La Mancha; Arturo S. Garcia: University of Castilla-La Mancha; Pascual Gonzalez: University of Castilla-La Mancha,
P1133,Reimagining TaxiVis through an Immersive Space-Time Cube metaphor and reflecting on potential benefits of Immersive Analytics for urban data exploration,Jorge Wagner,jawfilho@inf.ufrgs.br,"To explore how a traditional visualization system could be adapted into an immersive framework, and how it could benefit from this, we decided to revisit a landmark paper presented ten years ago at IEEE VIS. TaxiVis, by Ferreira et al., enabled interactive spatio-temporal querying of a large dataset of taxi trips in New York City. Here, we reimagine how TaxiVis’ functionalities could be implemented and extended in a 3D immersive environment. Through reporting on our experience, and on the vision and reasoning behind our design decisions, we hope to contribute to the debate on how conventional and immersive visualization paradigms can complement each other and also how the exploration of urban datasets can be facilitated in the coming years.",Jorge Wagner: Federal University of Rio Grande do Sul; Claudio Silva: New York University; Wolfgang Stuerzlinger: Simon Fraser University; Luciana Nedel: Federal University of Rio Grande do Sul (UFRGS),
P2117,The Effect of Directional Airflow toward Vection and Cybersickness,Seunghoon Park,qkrtmdgns23@korea.ac.kr,"Our study investigates the impact of directional airflow on cybersickness and user experience in virtual navigation. Findings reveal that airflow aligned with navigation enhances vection, while inconsistent airflow has no effect. However, airflow's refreshing quality significantly reduces cybersickness overall, regardless of its direction. These insights can guide the design of more dynamic and comfortable virtual navigation experiences",Seunghoon Park: Korea University; Seungwoo Son: Korea University; Jungha Kim: Korea University; Gerard Jounghyun Kim: Korea University,
P1872,Evaluating the Effect of Binaural Auralization on Audiovisual Plausibility and Communication Behavior in Virtual Reality,Felix Immohr,felix.immohr@tu-ilmenau.de,"Spatial audio has been shown to positively impact user experience in traditional communication media and presence in single-user VR. This work further investigates whether spatial audio benefits immersive communication scenarios. We present a study in which dyads communicate in VR under different auralization and scene arrangement conditions. A novel task is designed to increase the relevance of spatial hearing. Results are obtained through social presence and plausibility questionnaires, and through conversational and behavioral analysis. Although participants are shown to favor binaural over diotic audio in a direct comparison, no significant differences were observed from the other presented measures.",Felix Immohr: Technische Universität Ilmenau; Gareth Rendle: Bauhaus-Universität Weimar; Anton Benjamin Lammert: Bauhaus-Universität Weimar; Annika Neidhardt: University of Surrey; Victoria Meyer Zur Heyde: Technische Universität Ilmenau; Bernd Froehlich: Bauhaus-Universität Weimar; Alexander Raake: TU Ilmenau,
P1281,Universal Access for Social XR Across Devices: The Impact of Immersion on the Experience in Asymmetric Virtual Collaboration,Christian Merz,christian.merz@uni-wuerzburg.de,"This work examines how immersion in extended reality (XR) affects user experience (UX), focusing on presence, self-perception, other perception, and task perception in social XR contexts. We tested three device configurations in an asymmetric collaborative sorting task. Findings show that presence and self-perception, i.e., embodiment were lower with lower immersion. However, other-perception, i.e., co-presence and social presence were independent of immersion level. Our results suggest that while device choice within asymmetric collaborative XR influences self-perception and presence, it does not significantly impact other key UX aspects like other-perception, supporting the potential for universal access in social XR experiences.",Christian Merz: University of Würzburg; Christopher Göttfert: University of Würzburg; Carolin Wienrich: University of Würzburg; Marc Erich Latoschik: University of Würzburg,
P2106,Neural Bokeh: Learning Lens Blur for Computational Videography and Out-of-Focus Mixed Reality,David Mandl,mandl@icg.tugraz.at,"We present Neural Bokeh, a deep learning approach for synthesizing convincing out-of-focus effects with applications in Mixed Reality (MR) image and video compositing. Unlike existing approaches that solely learn the amount of blur for out-of-focus areas, our approach captures the overall characteristic of the bokeh to enable the seamless integration of rendered scene content into real images, ensuring a consistent lens blur over the resulting MR composition. Our method learns spatially varying blur shapes, which mimics the characteristics of the physical lens. We envision a variety of applications, including visual enhancement of image and video compositing containing creative utilization of out-of-focus effects.",David Mandl: Graz University of Technology; Shohei Mori: Graz University of Technology; Peter Mohr: Graz University of Technology; Yifan (Evan) Peng: The University of Hong Kong; Tobias Langlotz: University of Otago; Dieter Schmalstieg: Graz University of Technology; Denis Kalkofen: Graz University of Technology,
P1551,MagicMap: Enhancing Indoor Navigation Experience in VR Museums,Dr Yue Li,yue.li@xjtlu.edu.cn,"We present MagicMap, a novel navigation system for VR museums. It features a museum WiM attached to a 2D mini-map, supporting scalable map navigation and personal meaning-making through the annotations of museum artifacts and the recording of visiting traces. Translating the curatorial principles of museum visiting into a hierarchical menu layout, MagicMap demonstrates the ability to support prolonged engagement in VR museums and reduces users’ perceived workload in wayfinding. Our findings have implications for the future design of navigation systems in VR museums and complex indoor environments.",Xueqi Wang: Xi'an Jiaotong-Liverpool University; Yue Li: Xi'an Jiaotong-Liverpool University; Hai-Ning Liang: Xi'an Jiaotong-Liverpool University,
P1803,AMMA: Adaptive Multimodal Assistants Through Automated State Tracking and User Model-Directed Guidance Planning,Jackie (Junrui) Yang,jackiey@stanford.edu,"Novel technologies such as augmented reality and computer perception lay the foundation for smart task guidance assistants. However, real-world interaction requires adaptation to users’ mistakes, environments, and communication preferences. We propose Adaptive Multimodal Assistants (AMMA), a software architecture for adaptive guidance interfaces with a user action state tracker generated from step-by-step instructions and a guidance planner based on an adapting user model. With this, AMMA can adapt its guidance orders and modalities on the fly. We showed the viability of AMMA in an adaptive cooking assistant in VR. A user study with it showed that AMMA can reduce the task completion time and manual communication methods changes.",Jackie (Junrui) Yang: Stanford University; Leping Qiu: University of Toronto; Emmanuel Angel Corona-Moreno: Stanford University; Louisa Shi: Tsinghua University; Hung Bui: Stanford University; Monica Lam: Stanford University; James A. Landay: Stanford University,
P1137,Retinotopic Foveated Rendering,Yan Zhang,yan-zh@sjtu.edu.cn,"Existing foveated rendering (FR) works based on the radially symmetric regression model of human visual acuity. However, horizontal-vertical asymmetry and vertical meridian asymmetry of the human visual system have been evidenced by retinotopy research of neuroscience, suggesting the radially asymmetric regression of visual acuity. In this paper, we begin with the fMRI data and then introduce a radially asymmetric regression model for leveraging the rendering precision of FR. Our user study demonstrates retinotopic foveated rendering (RFR) provides participants with perceptually equal image quality compared to typical FR methods while reducing fragments shading by 27.2% averagely, leading to the acceleration of 1/6 for graphics rendering.",Yan Zhang: Shanghai Jiao Tong University; Keyao You: Shanghai Jiao Tong University; Xiaodan Hu: NAIST; Hangyu Zhou: Shanghai Jiao Tong University; Kiyoshi Kiyokawa: Nara Institute of Science and Technology; Xubo Yang: SHANGHAI JIAO TONG UNIVERSITY,
P1733,BaggingHook: Selecting Moving Targets by Pruning Distractors Away for Intention-Prediction Heuristics in Dense 3D Environments,Paolo Boffi,paolo.boffi@polimi.it,"This study presents two novel selection techniques: BaggingHook and AutoBaggingHook, based on distractor pruning and built upon the Hook intention-prediction heuristic. Our techniques reduce the number of targets in the environment, making them semi-transparent, to expedite heuristic convergence and reduce occlusion. BaggingHook allows manual distractor pruning, while AutoBaggingHook employs automated, score-based pruning. Results from a user study that compared both techniques to the Hook baseline show AutoBaggingHook was the fastest, while BaggingHook was preferred by most users for its greater user control. This work highlights the benefits of varying inputs in intention-prediction heuristics to improve performance and user experience.",Paolo Boffi: King Abdullah University of Science and Technology (KAUST); Alexandre Kouyoumdjian: King Abdullah University of Science and Technology; Manuela Waldner: TU Wien; Pier Luca Lanzi: Politecnico di Milano; Ivan Viola: King Abdullah University of Science and Technology,
P1841,illumotion: An Optical-illusion-based VR Locomotion Technique for Long-Distance 3D Movement,Zackary P. T. Sin,csptsin@comp.polyu.edu.hk,"Locomotion has a marked impact on user experience in VR, but common to-go techniques such as steering and teleportation have their limitations. Particularly, steering is prone to cybersickness, while teleportation trades presence for mitigating cybersickness. Inspired by how we manipulate a picture on a phone, we propose illumotion, an optical-illusion-based method that, we believe, can be an alternative for locomotion. Instead of zooming in a picture by pinching two fingers, we can move forward by “zooming” toward part of the 3D virtual scene with pinched hands. Results show that, compared with either teleportation, steering or both, illumotion has better performance, presence, usability, user experience and cybersickness alleviation.",Zackary P. T. Sin: The Hong Kong Polytechnic University; Ye Jia: The Hong Kong Polytechnic University; Chen Li: The Hong Kong Polytechnic University ; Hong Va Leong: The Hong Kong Polytechnic University; qing li: The Hong Kong Polytechnic University; Peter H.F. Ng: The Hong Kong Polytechnic University,
P1107,When vision lies - Navigating virtual environments with unreliable visual information,Shachar Maidenbaum,shachar.maidenbaum@gmail.com,"What happens to human navigation when vision becomes actively unreliable? Will it impair user performance, be suppressed, or be used advantageously? While such scenarios are rare in the real world, this question has important implications for multisensory integration in extended reality applications - e.g. virtual walls that a user sees but can walk through. We tested this using mazes with reliable and unreliable sensory channels, which induced ghost walls (perceived but not boundaries) and invisible walls (no perception but boundaries). We found that unreliable vision led to better performance than without vision but that unreliable audition was suppressed.",Eden Or: Ben Gurion University; Shachar Maidenbaum: Ben Gurion University,
P1787,Beyond Slideshows–Investigating the Impact of Immersive Virtual Reality on Science Learning,Fabian Froehlich,ff2093@nyu.edu,"We investigated the effectiveness of immersive VR (VR) for science learning by comparing an VR environment with traditional learning. One group learned about cell biology with a headmounted display using interactive simulation modules in VR, the other with a slideshow presenting the same materials. This study focused on the research question: ""Does learning in immersive VR designed to take advantage of VR affordances lead to higher learning outcomes and affective outcomes compared to traditional instruction?"" In an experimental between-subject control group design (N= 63), we measured students’ recall and affect. Results indicate that the VR group scored significantly higher than the control group on some dimensions of the post assessment.",Fabian Froehlich: NYU; Chris Hovey: NYU; Sameen Reza: New York University; Jan Plass: New York University,
P1513,Influence of user’s body in olfactory virtual environment generated by real-time CFD,Masafumi Uda,uda.m.aa@m.titech.ac.jp,"We have developed a virtual olfactory environment using an olfactory display and computational fluid dynamics (CFD) simulation. Although CFD can calculate the odor distribution in the complicated geometry, its computational cost was expensive and did not work in real time in the previous study. In this study, real-time CFD based on GPU calculation was introduced to generate real-time olfactory VR environment. We investigated influence of the user's body with its location and orientation changing irregularly. The experimental result indicates the usefulness of considering the effect of the user's body since we cannot avoid that influence.",Masafumi Uda: School of Engineering; Takamichi Nakamoto: Tokyo Institute of Technology,
P1386,CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency,Hanxin Zhu,hanxinzhu@mail.ustc.edu.cn,"We propose CMC, a novel method for few-shot novel view synthesis via cross-view multiplane consistency. Our key insight is that by forcing the same spatial points to be sampled repeatedly in different input views, we can strengthen the interactions between views and therefore alleviate the overfitting problem. To achieve this, we build the neural networks on layered representations (\textit{i.e.}, multiplane images), and the sampling point can thus be resampled on multiple discrete planes. Furthermore, to regularize the unseen target views, we constrain the rendered colors and depths from different input views to be the same. Experiments demonstrate that our proposed method can achieve better synthesis quality over state-of-the-art methods",Hanxin Zhu: University of Science and Technology of China; Zhibo Chen: University of Science and Technology of China,
P1282,NHVC: Neural Holographic Video Compression with Scalable Architecture,Hyunmin Ban,hmban1996@khu.ac.kr,"This paper presents a scalable, end-to-end trainable model for efficient phase hologram video generation and compression. The proposed model, Neural Holographic Video Compression (NHVC), combines an auto-encoder-based phase hologram generator with a latent coder and dual hyper-prior coders, achieving task scalability across hologram generation and compression. It significantly outperforms existing methods, evidenced by a 75.6% BD-Rate reduction over 'HoloNet + VVC' and delivering high-quality reconstruction. NHVC addresses key design questions in neural compression, offering solutions for motion estimation in the phase domain and variable rate support.",Hyunmin Ban: Kyung Hee University; Seungmi Choi: Kyung Hee University; Jun Yeong Cha: Kyung Hee University; Yeongwoong Kim: Kyung Hee University; Hui Yong Kim: Kyung Hee Univ.,
P2097,Googly Eyes: Exploring Effects of Displaying User’s Eye Movements Outward on a VR Head-Mounted Display on User Experience,Evren Bozgeyikli,rboz@arizona.edu,"Head-mounted displays (HMDs) in virtual reality (VR) occlude the upper face of the wearing users, leading to decreased nonverbal communication cues toward outside users. In this paper, we discuss Googly Eyes, a high-fidelity prototype that displays an illustration of the HMD-wearing user’s eyes in real-time in front of an HMD. We designed and developed a collaborative asymmetrical co-located task performed by an HMD-wearing user and a non-HMD (tablet) user, and we conducted a between-subjects user study where we compared the Googly Eyes with a baseline HMD experience without any external eye depiction. In this paper, we discuss the system, task, user study details, and results along with implications for future studies.","Evren Bozgeyikli: University of Arizona; Lal ""Lila"" Bozgeyikli: University of Arizona; Victor Gomes: University of Arizona",
P1815,A Scientometric History of IEEE VR,Richard Skarbez,skarbez@gmail.com,"As of IEEE VR 2023, there have been 30 installments of the IEEE Virtual Reality conference (VR) or its predecessor, the Virtual Reality Annual International Symposium (VRAIS). As such, it seems an opportune time to reflect on the intellectual history of the conference, and by extension, the VR research community. This article uses scientometric techniques to undertake such an intellectual history, using co-word analysis and citation analysis to identify core themes and trends in VR research over time. We identify the papers that have stood the test of time, the most esteemed authors and researchers in the IEEE VR community, and the topics that have shaped our field to date.",Richard Skarbez: La Trobe University; Dai Jiang: La Trobe University,
P1816,StageAR: Markerless Mobile Phone Localization for AR in Live Events,Tao Jin,taojin@andrew.cmu.edu,"Localizing mobile phone users for AR in dynamic theater and concert settings is challenging due to changing staging and lighting. Visual markers disrupt aesthetics, and static maps aren't reliable amidst visual changes. Our study explores techniques using sparse infrastructure to adapt to environmental changes for robust AR tracking. Our basic method uses fixed cameras to filter unreliable feature points from a static model. For tougher settings, our technique generates dynamic 3D feature maps for real-time scene adaptation, allowing precise mobile phone localization without markers. We assess StageAR's various techniques. Our top method rivals the accuracy of large static markers while remaining unseen by the audience.",Tao Jin: Carnegie Mellon University; Shengxi Wu: Carnegie Mellon University; Mallesham Dasari: Carnegie Mellon University; Kittipat Apicharttrisorn: Nokia Bell Labs; Anthony Rowe: Carnegie Mellon University,
P1331,Reaching Between Worlds: Calibration and Transfer of Perceived Affordances from Virtual to Real Environments,Holly C Gagnon,holly.gagnon@psych.utah.edu,"Accurate perception of action capabilities (affordance perception) is essential for interaction with real and virtual environments, but it is unknown what specific types of feedback are needed to improve affordance perception. This study examined whether perceived horizontal reachability in VR and AR improved with feedback, and if improvement transferred to the real world. Three types of feedback were studied: exploratory behavior, static outcome, and action outcome feedback. Our results indicate that exploratory behavior is sufficient for improved perceived reachability in VR, but in AR outcome feedback is necessary. In VR and AR outcome feedback was required for improvement in perceived reachability to transfer to the real world.",Holly C Gagnon: University of Utah; Hunter C Finney: University of Utah; Jeanine Stefanucci: University of Utah; Bobby Bodenheimer: Vanderbilt University; Sarah Creem-Regehr: University of Utah,
P1312,VollyNaut: Pioneering Immersive Training for Inclusive Sitting Volleyball Skill Development,Ut Gong,jojogong3736@gmail.com,"Participating in sports provides disabled individuals with social inclusion, health benefits, and confidence. Yet, popular para-sports such as sitting volleyball face limited access due to scarce dedicated courts. Addressing this, we conducted a pioneering VR para-sports training study and developed VolleyNaut, a VR system created in collaboration with professional coaches to realistically simulate daily training drills and ball pitches. Our user study included college volleyball players, as well as national sitting volleyball athletes and coaches, to assess VolleyNaut's engagement and training effectiveness. The results, incorporating both quantitative and qualitative data, showed consistently positive feedback across all groups.",Ut Gong: Zhejiang University; Hanze Jia: Zhejiang University; Yujie Wang: Zhejiang University; Tan Tang: Zhejiang University; Xiao Xie: Zhejiang University; Yingcai Wu: Zhejiang University,
P2102,Influence of Virtual Shoe Formality on Gait and Cognitive Performance in a VR Walking Task,Sebastian Oberdörfer,sebastian.oberdoerfer@uni-wuerzburg.de,"Shoes come in various degrees of formality and their structure can affect human gait. In our study, we embody 39 participants with a generic avatar of the user's gender wearing three different pairs of shoes as within condition. The shoes differ in degree of formality. We measure the gait during a 2-minute walking task during which participants wore the same real shoe and assess selective attention using the Stroop task. Our results show significant differences in gait between the tested virtual shoe pairs. We found small effects between the three shoe conditions with respect to selective attention. However, we found no significant differences with respect to correct items and response time in the Stroop task.",Sebastian Oberdörfer: University of Würzburg; Sandra Birnstiel: Friedrich-Alexander-Universität Erlangen-Nürnberg; Marc Erich Latoschik: University of Würzburg,
P1667,Immersive 3D Medical Visualization in Virtual Reality using Stereoscopic Volumetric Path Tracing,Jose A. Iglesias-Guitian,j.iglesias.guitian@udc.es,"Scientific visualization using Monte Carlo (MC) volumetric path tracing and physically-based lighting is applied in medical contexts under the term cinematic rendering (CR), and it benefits healthcare professionals with photorealistic 3D reconstructions. However, given its high computational cost, CR deployment in VR finds significant issues, such as limited interactivity and temporal flicker caused by noisy MC renderings. We present an immersive 3D medical visualization system capable of generating photorealistic and fully interactive stereoscopic renderings on head-mounted display (HMD) devices. Our method extends previous linear regression denoising to enable real-time stereoscopic volumetric path tracing within VR environments.",Javier Taibo: Universidade da Coruña; Jose A. Iglesias-Guitian: Universidade da Coruña,
P1882,Evaluating Plausible Preference of Body-Centric Locomotion using Subjective Matching in Virtual Reality,BoYu Gao,bygao@jnu.edu.cn,"Body-centric locomotion in Virtual Reality (VR) involves multiple factors, including the point of view, avatar representations, tracked body parts for locomotion control and transfer functions that map body movement to the displacement of the virtual viewpoint. This study employed the subjective matching method to evaluate the sense of plausible walking experience during body-centric locomotion in VR. A virtual locomotion experiment with these five factors based on subjective matching was conducted. The results could serve as the guidelines for virtual locomotion experience design that involves combinations of multiple factors and can help achieve a plausible walking experience in VR.",BoYu Gao: Jinan University; Haojun Zheng: Jinan University; Jingbo Zhao: China Agricultural University; Huawei Tu: La Trobe University; Hyungseok Kim: Konkuk University; Henry Been-Lirn Duh: Hong Kong Polytechnic University,